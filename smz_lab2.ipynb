{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6820742a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.nn import Conv3d\n",
    "from torch import from_numpy\n",
    "from numpy.testing import assert_array_equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c16490e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3d(input, weight, bias, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1,\n",
    "                   bias_enabled=True, padding_mode='zeros'):\n",
    "    batches = len(input)\n",
    "    out = []\n",
    "\n",
    "    for b in range(batches):\n",
    "        d_in, h_in, w_in = (input[b]).shape[1], (input[b]).shape[2], (input[b]).shape[3]\n",
    "\n",
    "        kernel_size = (kernel_size, kernel_size, kernel_size) if not isinstance(kernel_size, tuple) else kernel_size\n",
    "        stride = (stride, stride, stride) if not isinstance(stride, tuple) else stride\n",
    "        dilation = (dilation, dilation, dilation) if not isinstance(dilation, tuple) else dilation\n",
    "\n",
    "        padding = padding if isinstance(padding, tuple) else (padding, padding, padding)\n",
    "\n",
    "        d_out = int((d_in + 2 * padding[0] - dilation[0] * (kernel_size[0] - 1) - 1) / stride[0] + 1)\n",
    "        h_out = int((h_in + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1) / stride[1] + 1)\n",
    "        w_out = int((w_in + 2 * padding[2] - dilation[2] * (kernel_size[2] - 1) - 1) / stride[2] + 1)\n",
    "\n",
    "        out.append(np.zeros((out_channels, d_out, h_out, w_out)))\n",
    "\n",
    "        for c_out in range(out_channels):\n",
    "            for z_out in range(d_out):\n",
    "                for y_out in range(h_out):\n",
    "                    for x_out in range(w_out):\n",
    "                        summation = 0\n",
    "                        for c_in in range(in_channels):\n",
    "                            for kernel_z in range(kernel_size[0]):\n",
    "                                for kernel_y in range(kernel_size[1]):\n",
    "                                    for kernel_x in range(kernel_size[2]):\n",
    "                                        z_in = z_out * stride[0] + kernel_z * dilation[0] - padding[0]\n",
    "                                        y_in = y_out * stride[1] + kernel_y * dilation[1] - padding[1]\n",
    "                                        x_in = x_out * stride[2] + kernel_x * dilation[2] - padding[2]\n",
    "                                        if 0 <= z_in < d_in and 0 <= y_in < h_in and 0 <= x_in < w_in:\n",
    "                                            summation += input[b][c_in][z_in][y_in][x_in] * \\\n",
    "                                                          weight[c_out][c_in][kernel_z][kernel_y][kernel_x]\n",
    "                                        elif padding_mode == 'replicate':\n",
    "                                            z_in = max(0, min(z_in, d_in - 1))\n",
    "                                            y_in = max(0, min(y_in, h_in - 1))\n",
    "                                            x_in = max(0, min(x_in, w_in - 1))\n",
    "                                            summation += input[b][c_in][z_in][y_in][x_in] * \\\n",
    "                                                          weight[c_out][c_in][kernel_z][kernel_y][kernel_x]\n",
    "\n",
    "                        out[b][c_out][z_out][y_out][x_out] = summation + (bias[c_out] if bias_enabled else 0)\n",
    "\n",
    "    return np.array(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dc1becf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero inputs and weights\n",
      "Custom\n",
      "[[[[[0. 0. 0.]\n",
      "    [0. 0. 0.]\n",
      "    [0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0.]\n",
      "    [0. 0. 0.]\n",
      "    [0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0.]\n",
      "    [0. 0. 0.]\n",
      "    [0. 0. 0.]]]\n",
      "\n",
      "\n",
      "  [[[0. 0. 0.]\n",
      "    [0. 0. 0.]\n",
      "    [0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0.]\n",
      "    [0. 0. 0.]\n",
      "    [0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0.]\n",
      "    [0. 0. 0.]\n",
      "    [0. 0. 0.]]]]]\n",
      "Pytorch\n",
      "[[[[[0. 0. 0.]\n",
      "    [0. 0. 0.]\n",
      "    [0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0.]\n",
      "    [0. 0. 0.]\n",
      "    [0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0.]\n",
      "    [0. 0. 0.]\n",
      "    [0. 0. 0.]]]\n",
      "\n",
      "\n",
      "  [[[0. 0. 0.]\n",
      "    [0. 0. 0.]\n",
      "    [0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0.]\n",
      "    [0. 0. 0.]\n",
      "    [0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0.]\n",
      "    [0. 0. 0.]\n",
      "    [0. 0. 0.]]]]]\n",
      "Default data\n",
      "Custom\n",
      "[[[[[ 88. 104. 120.]\n",
      "    [120. 136. 152.]\n",
      "    [152. 168. 184.]]\n",
      "\n",
      "   [[ 88. 104. 120.]\n",
      "    [120. 136. 152.]\n",
      "    [152. 168. 184.]]\n",
      "\n",
      "   [[ 88. 104. 120.]\n",
      "    [120. 136. 152.]\n",
      "    [152. 168. 184.]]]\n",
      "\n",
      "\n",
      "  [[[ 88. 104. 120.]\n",
      "    [120. 136. 152.]\n",
      "    [152. 168. 184.]]\n",
      "\n",
      "   [[ 88. 104. 120.]\n",
      "    [120. 136. 152.]\n",
      "    [152. 168. 184.]]\n",
      "\n",
      "   [[ 88. 104. 120.]\n",
      "    [120. 136. 152.]\n",
      "    [152. 168. 184.]]]]]\n",
      "Pytorch\n",
      "[[[[[ 88. 104. 120.]\n",
      "    [120. 136. 152.]\n",
      "    [152. 168. 184.]]\n",
      "\n",
      "   [[ 88. 104. 120.]\n",
      "    [120. 136. 152.]\n",
      "    [152. 168. 184.]]\n",
      "\n",
      "   [[ 88. 104. 120.]\n",
      "    [120. 136. 152.]\n",
      "    [152. 168. 184.]]]\n",
      "\n",
      "\n",
      "  [[[ 88. 104. 120.]\n",
      "    [120. 136. 152.]\n",
      "    [152. 168. 184.]]\n",
      "\n",
      "   [[ 88. 104. 120.]\n",
      "    [120. 136. 152.]\n",
      "    [152. 168. 184.]]\n",
      "\n",
      "   [[ 88. 104. 120.]\n",
      "    [120. 136. 152.]\n",
      "    [152. 168. 184.]]]]]\n",
      "Random data\n",
      "Custom\n",
      "[[[[[3. 3. 4. 5.]\n",
      "    [4. 4. 4. 5.]\n",
      "    [4. 3. 3. 3.]\n",
      "    [4. 3. 3. 3.]]\n",
      "\n",
      "   [[4. 4. 4. 4.]\n",
      "    [4. 4. 4. 3.]\n",
      "    [4. 4. 4. 4.]\n",
      "    [4. 4. 4. 5.]]\n",
      "\n",
      "   [[4. 4. 4. 3.]\n",
      "    [4. 5. 4. 4.]\n",
      "    [4. 6. 6. 5.]\n",
      "    [4. 5. 4. 5.]]\n",
      "\n",
      "   [[4. 4. 4. 3.]\n",
      "    [4. 4. 4. 4.]\n",
      "    [5. 6. 6. 5.]\n",
      "    [6. 5. 4. 4.]]]\n",
      "\n",
      "\n",
      "  [[[4. 4. 5. 5.]\n",
      "    [4. 4. 5. 6.]\n",
      "    [5. 4. 4. 4.]\n",
      "    [4. 3. 4. 3.]]\n",
      "\n",
      "   [[5. 4. 6. 5.]\n",
      "    [4. 4. 5. 5.]\n",
      "    [5. 5. 5. 4.]\n",
      "    [4. 4. 5. 6.]]\n",
      "\n",
      "   [[5. 4. 5. 4.]\n",
      "    [5. 5. 5. 3.]\n",
      "    [5. 7. 6. 5.]\n",
      "    [4. 6. 5. 5.]]\n",
      "\n",
      "   [[5. 5. 4. 4.]\n",
      "    [5. 5. 5. 4.]\n",
      "    [5. 6. 6. 6.]\n",
      "    [7. 6. 5. 5.]]]]]\n",
      "Pytorch\n",
      "[[[[[3. 3. 4. 5.]\n",
      "    [4. 4. 4. 5.]\n",
      "    [4. 3. 3. 3.]\n",
      "    [4. 3. 3. 3.]]\n",
      "\n",
      "   [[4. 4. 4. 4.]\n",
      "    [4. 4. 4. 3.]\n",
      "    [4. 4. 4. 4.]\n",
      "    [4. 4. 4. 5.]]\n",
      "\n",
      "   [[4. 4. 4. 3.]\n",
      "    [4. 5. 4. 4.]\n",
      "    [4. 6. 6. 5.]\n",
      "    [4. 5. 4. 5.]]\n",
      "\n",
      "   [[4. 4. 4. 3.]\n",
      "    [4. 4. 4. 4.]\n",
      "    [5. 6. 6. 5.]\n",
      "    [6. 5. 4. 4.]]]\n",
      "\n",
      "\n",
      "  [[[4. 4. 5. 5.]\n",
      "    [4. 4. 5. 6.]\n",
      "    [5. 4. 4. 4.]\n",
      "    [4. 3. 4. 3.]]\n",
      "\n",
      "   [[5. 4. 6. 5.]\n",
      "    [4. 4. 5. 5.]\n",
      "    [5. 5. 5. 4.]\n",
      "    [4. 4. 5. 6.]]\n",
      "\n",
      "   [[5. 4. 5. 4.]\n",
      "    [5. 5. 5. 3.]\n",
      "    [5. 7. 6. 5.]\n",
      "    [4. 6. 5. 5.]]\n",
      "\n",
      "   [[5. 5. 4. 4.]\n",
      "    [5. 5. 5. 4.]\n",
      "    [5. 6. 6. 6.]\n",
      "    [7. 6. 5. 5.]]]]]\n",
      "Stride test:\n",
      "Custom\n",
      "[[[[[ 88. 120.]\n",
      "    [152. 184.]]\n",
      "\n",
      "   [[ 88. 120.]\n",
      "    [152. 184.]]]\n",
      "\n",
      "\n",
      "  [[[ 88. 120.]\n",
      "    [152. 184.]]\n",
      "\n",
      "   [[ 88. 120.]\n",
      "    [152. 184.]]]]]\n",
      "Pytorch\n",
      "[[[[[ 88. 120.]\n",
      "    [152. 184.]]\n",
      "\n",
      "   [[ 88. 120.]\n",
      "    [152. 184.]]]\n",
      "\n",
      "\n",
      "  [[[ 88. 120.]\n",
      "    [152. 184.]]\n",
      "\n",
      "   [[ 88. 120.]\n",
      "    [152. 184.]]]]]\n",
      "Kernel size test:\n",
      "Custom\n",
      "[[[[[ 88. 104. 120.]\n",
      "    [120. 136. 152.]\n",
      "    [152. 168. 184.]]\n",
      "\n",
      "   [[ 88. 104. 120.]\n",
      "    [120. 136. 152.]\n",
      "    [152. 168. 184.]]\n",
      "\n",
      "   [[ 88. 104. 120.]\n",
      "    [120. 136. 152.]\n",
      "    [152. 168. 184.]]]\n",
      "\n",
      "\n",
      "  [[[ 88. 104. 120.]\n",
      "    [120. 136. 152.]\n",
      "    [152. 168. 184.]]\n",
      "\n",
      "   [[ 88. 104. 120.]\n",
      "    [120. 136. 152.]\n",
      "    [152. 168. 184.]]\n",
      "\n",
      "   [[ 88. 104. 120.]\n",
      "    [120. 136. 152.]\n",
      "    [152. 168. 184.]]]]]\n",
      "Pytorch\n",
      "[[[[[ 88. 104. 120.]\n",
      "    [120. 136. 152.]\n",
      "    [152. 168. 184.]]\n",
      "\n",
      "   [[ 88. 104. 120.]\n",
      "    [120. 136. 152.]\n",
      "    [152. 168. 184.]]\n",
      "\n",
      "   [[ 88. 104. 120.]\n",
      "    [120. 136. 152.]\n",
      "    [152. 168. 184.]]]\n",
      "\n",
      "\n",
      "  [[[ 88. 104. 120.]\n",
      "    [120. 136. 152.]\n",
      "    [152. 168. 184.]]\n",
      "\n",
      "   [[ 88. 104. 120.]\n",
      "    [120. 136. 152.]\n",
      "    [152. 168. 184.]]\n",
      "\n",
      "   [[ 88. 104. 120.]\n",
      "    [120. 136. 152.]\n",
      "    [152. 168. 184.]]]]]\n",
      "Padding test:\n",
      "Custom\n",
      "[[[[[  2.   6.  10.  14.   8.]\n",
      "    [ 20.  44.  52.  60.  32.]\n",
      "    [ 28.  60.  68.  76.  40.]\n",
      "    [ 36.  76.  84.  92.  48.]\n",
      "    [ 26.  54.  58.  62.  32.]]\n",
      "\n",
      "   [[  4.  12.  20.  28.  16.]\n",
      "    [ 40.  88. 104. 120.  64.]\n",
      "    [ 56. 120. 136. 152.  80.]\n",
      "    [ 72. 152. 168. 184.  96.]\n",
      "    [ 52. 108. 116. 124.  64.]]\n",
      "\n",
      "   [[  4.  12.  20.  28.  16.]\n",
      "    [ 40.  88. 104. 120.  64.]\n",
      "    [ 56. 120. 136. 152.  80.]\n",
      "    [ 72. 152. 168. 184.  96.]\n",
      "    [ 52. 108. 116. 124.  64.]]\n",
      "\n",
      "   [[  4.  12.  20.  28.  16.]\n",
      "    [ 40.  88. 104. 120.  64.]\n",
      "    [ 56. 120. 136. 152.  80.]\n",
      "    [ 72. 152. 168. 184.  96.]\n",
      "    [ 52. 108. 116. 124.  64.]]\n",
      "\n",
      "   [[  2.   6.  10.  14.   8.]\n",
      "    [ 20.  44.  52.  60.  32.]\n",
      "    [ 28.  60.  68.  76.  40.]\n",
      "    [ 36.  76.  84.  92.  48.]\n",
      "    [ 26.  54.  58.  62.  32.]]]\n",
      "\n",
      "\n",
      "  [[[  2.   6.  10.  14.   8.]\n",
      "    [ 20.  44.  52.  60.  32.]\n",
      "    [ 28.  60.  68.  76.  40.]\n",
      "    [ 36.  76.  84.  92.  48.]\n",
      "    [ 26.  54.  58.  62.  32.]]\n",
      "\n",
      "   [[  4.  12.  20.  28.  16.]\n",
      "    [ 40.  88. 104. 120.  64.]\n",
      "    [ 56. 120. 136. 152.  80.]\n",
      "    [ 72. 152. 168. 184.  96.]\n",
      "    [ 52. 108. 116. 124.  64.]]\n",
      "\n",
      "   [[  4.  12.  20.  28.  16.]\n",
      "    [ 40.  88. 104. 120.  64.]\n",
      "    [ 56. 120. 136. 152.  80.]\n",
      "    [ 72. 152. 168. 184.  96.]\n",
      "    [ 52. 108. 116. 124.  64.]]\n",
      "\n",
      "   [[  4.  12.  20.  28.  16.]\n",
      "    [ 40.  88. 104. 120.  64.]\n",
      "    [ 56. 120. 136. 152.  80.]\n",
      "    [ 72. 152. 168. 184.  96.]\n",
      "    [ 52. 108. 116. 124.  64.]]\n",
      "\n",
      "   [[  2.   6.  10.  14.   8.]\n",
      "    [ 20.  44.  52.  60.  32.]\n",
      "    [ 28.  60.  68.  76.  40.]\n",
      "    [ 36.  76.  84.  92.  48.]\n",
      "    [ 26.  54.  58.  62.  32.]]]]]\n",
      "Pytorch\n",
      "[[[[[  2.   6.  10.  14.   8.]\n",
      "    [ 20.  44.  52.  60.  32.]\n",
      "    [ 28.  60.  68.  76.  40.]\n",
      "    [ 36.  76.  84.  92.  48.]\n",
      "    [ 26.  54.  58.  62.  32.]]\n",
      "\n",
      "   [[  4.  12.  20.  28.  16.]\n",
      "    [ 40.  88. 104. 120.  64.]\n",
      "    [ 56. 120. 136. 152.  80.]\n",
      "    [ 72. 152. 168. 184.  96.]\n",
      "    [ 52. 108. 116. 124.  64.]]\n",
      "\n",
      "   [[  4.  12.  20.  28.  16.]\n",
      "    [ 40.  88. 104. 120.  64.]\n",
      "    [ 56. 120. 136. 152.  80.]\n",
      "    [ 72. 152. 168. 184.  96.]\n",
      "    [ 52. 108. 116. 124.  64.]]\n",
      "\n",
      "   [[  4.  12.  20.  28.  16.]\n",
      "    [ 40.  88. 104. 120.  64.]\n",
      "    [ 56. 120. 136. 152.  80.]\n",
      "    [ 72. 152. 168. 184.  96.]\n",
      "    [ 52. 108. 116. 124.  64.]]\n",
      "\n",
      "   [[  2.   6.  10.  14.   8.]\n",
      "    [ 20.  44.  52.  60.  32.]\n",
      "    [ 28.  60.  68.  76.  40.]\n",
      "    [ 36.  76.  84.  92.  48.]\n",
      "    [ 26.  54.  58.  62.  32.]]]\n",
      "\n",
      "\n",
      "  [[[  2.   6.  10.  14.   8.]\n",
      "    [ 20.  44.  52.  60.  32.]\n",
      "    [ 28.  60.  68.  76.  40.]\n",
      "    [ 36.  76.  84.  92.  48.]\n",
      "    [ 26.  54.  58.  62.  32.]]\n",
      "\n",
      "   [[  4.  12.  20.  28.  16.]\n",
      "    [ 40.  88. 104. 120.  64.]\n",
      "    [ 56. 120. 136. 152.  80.]\n",
      "    [ 72. 152. 168. 184.  96.]\n",
      "    [ 52. 108. 116. 124.  64.]]\n",
      "\n",
      "   [[  4.  12.  20.  28.  16.]\n",
      "    [ 40.  88. 104. 120.  64.]\n",
      "    [ 56. 120. 136. 152.  80.]\n",
      "    [ 72. 152. 168. 184.  96.]\n",
      "    [ 52. 108. 116. 124.  64.]]\n",
      "\n",
      "   [[  4.  12.  20.  28.  16.]\n",
      "    [ 40.  88. 104. 120.  64.]\n",
      "    [ 56. 120. 136. 152.  80.]\n",
      "    [ 72. 152. 168. 184.  96.]\n",
      "    [ 52. 108. 116. 124.  64.]]\n",
      "\n",
      "   [[  2.   6.  10.  14.   8.]\n",
      "    [ 20.  44.  52.  60.  32.]\n",
      "    [ 28.  60.  68.  76.  40.]\n",
      "    [ 36.  76.  84.  92.  48.]\n",
      "    [ 26.  54.  58.  62.  32.]]]]]\n",
      "Padding test:\n",
      "Custom\n",
      "[[[[[  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]]\n",
      "\n",
      "   [[  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   2.   6.  10.  14.   8.   0.]\n",
      "    [  0.  20.  44.  52.  60.  32.   0.]\n",
      "    [  0.  28.  60.  68.  76.  40.   0.]\n",
      "    [  0.  36.  76.  84.  92.  48.   0.]\n",
      "    [  0.  26.  54.  58.  62.  32.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]]\n",
      "\n",
      "   [[  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   4.  12.  20.  28.  16.   0.]\n",
      "    [  0.  40.  88. 104. 120.  64.   0.]\n",
      "    [  0.  56. 120. 136. 152.  80.   0.]\n",
      "    [  0.  72. 152. 168. 184.  96.   0.]\n",
      "    [  0.  52. 108. 116. 124.  64.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]]\n",
      "\n",
      "   [[  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   4.  12.  20.  28.  16.   0.]\n",
      "    [  0.  40.  88. 104. 120.  64.   0.]\n",
      "    [  0.  56. 120. 136. 152.  80.   0.]\n",
      "    [  0.  72. 152. 168. 184.  96.   0.]\n",
      "    [  0.  52. 108. 116. 124.  64.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]]\n",
      "\n",
      "   [[  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   4.  12.  20.  28.  16.   0.]\n",
      "    [  0.  40.  88. 104. 120.  64.   0.]\n",
      "    [  0.  56. 120. 136. 152.  80.   0.]\n",
      "    [  0.  72. 152. 168. 184.  96.   0.]\n",
      "    [  0.  52. 108. 116. 124.  64.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]]\n",
      "\n",
      "   [[  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   2.   6.  10.  14.   8.   0.]\n",
      "    [  0.  20.  44.  52.  60.  32.   0.]\n",
      "    [  0.  28.  60.  68.  76.  40.   0.]\n",
      "    [  0.  36.  76.  84.  92.  48.   0.]\n",
      "    [  0.  26.  54.  58.  62.  32.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]]\n",
      "\n",
      "   [[  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]]]\n",
      "\n",
      "\n",
      "  [[[  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]]\n",
      "\n",
      "   [[  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   2.   6.  10.  14.   8.   0.]\n",
      "    [  0.  20.  44.  52.  60.  32.   0.]\n",
      "    [  0.  28.  60.  68.  76.  40.   0.]\n",
      "    [  0.  36.  76.  84.  92.  48.   0.]\n",
      "    [  0.  26.  54.  58.  62.  32.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]]\n",
      "\n",
      "   [[  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   4.  12.  20.  28.  16.   0.]\n",
      "    [  0.  40.  88. 104. 120.  64.   0.]\n",
      "    [  0.  56. 120. 136. 152.  80.   0.]\n",
      "    [  0.  72. 152. 168. 184.  96.   0.]\n",
      "    [  0.  52. 108. 116. 124.  64.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]]\n",
      "\n",
      "   [[  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   4.  12.  20.  28.  16.   0.]\n",
      "    [  0.  40.  88. 104. 120.  64.   0.]\n",
      "    [  0.  56. 120. 136. 152.  80.   0.]\n",
      "    [  0.  72. 152. 168. 184.  96.   0.]\n",
      "    [  0.  52. 108. 116. 124.  64.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]]\n",
      "\n",
      "   [[  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   4.  12.  20.  28.  16.   0.]\n",
      "    [  0.  40.  88. 104. 120.  64.   0.]\n",
      "    [  0.  56. 120. 136. 152.  80.   0.]\n",
      "    [  0.  72. 152. 168. 184.  96.   0.]\n",
      "    [  0.  52. 108. 116. 124.  64.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]]\n",
      "\n",
      "   [[  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   2.   6.  10.  14.   8.   0.]\n",
      "    [  0.  20.  44.  52.  60.  32.   0.]\n",
      "    [  0.  28.  60.  68.  76.  40.   0.]\n",
      "    [  0.  36.  76.  84.  92.  48.   0.]\n",
      "    [  0.  26.  54.  58.  62.  32.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]]\n",
      "\n",
      "   [[  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]]]]]\n",
      "Pytorch\n",
      "[[[[[  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]]\n",
      "\n",
      "   [[  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   2.   6.  10.  14.   8.   0.]\n",
      "    [  0.  20.  44.  52.  60.  32.   0.]\n",
      "    [  0.  28.  60.  68.  76.  40.   0.]\n",
      "    [  0.  36.  76.  84.  92.  48.   0.]\n",
      "    [  0.  26.  54.  58.  62.  32.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]]\n",
      "\n",
      "   [[  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   4.  12.  20.  28.  16.   0.]\n",
      "    [  0.  40.  88. 104. 120.  64.   0.]\n",
      "    [  0.  56. 120. 136. 152.  80.   0.]\n",
      "    [  0.  72. 152. 168. 184.  96.   0.]\n",
      "    [  0.  52. 108. 116. 124.  64.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]]\n",
      "\n",
      "   [[  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   4.  12.  20.  28.  16.   0.]\n",
      "    [  0.  40.  88. 104. 120.  64.   0.]\n",
      "    [  0.  56. 120. 136. 152.  80.   0.]\n",
      "    [  0.  72. 152. 168. 184.  96.   0.]\n",
      "    [  0.  52. 108. 116. 124.  64.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]]\n",
      "\n",
      "   [[  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   4.  12.  20.  28.  16.   0.]\n",
      "    [  0.  40.  88. 104. 120.  64.   0.]\n",
      "    [  0.  56. 120. 136. 152.  80.   0.]\n",
      "    [  0.  72. 152. 168. 184.  96.   0.]\n",
      "    [  0.  52. 108. 116. 124.  64.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]]\n",
      "\n",
      "   [[  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   2.   6.  10.  14.   8.   0.]\n",
      "    [  0.  20.  44.  52.  60.  32.   0.]\n",
      "    [  0.  28.  60.  68.  76.  40.   0.]\n",
      "    [  0.  36.  76.  84.  92.  48.   0.]\n",
      "    [  0.  26.  54.  58.  62.  32.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]]\n",
      "\n",
      "   [[  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]]]\n",
      "\n",
      "\n",
      "  [[[  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]]\n",
      "\n",
      "   [[  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   2.   6.  10.  14.   8.   0.]\n",
      "    [  0.  20.  44.  52.  60.  32.   0.]\n",
      "    [  0.  28.  60.  68.  76.  40.   0.]\n",
      "    [  0.  36.  76.  84.  92.  48.   0.]\n",
      "    [  0.  26.  54.  58.  62.  32.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]]\n",
      "\n",
      "   [[  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   4.  12.  20.  28.  16.   0.]\n",
      "    [  0.  40.  88. 104. 120.  64.   0.]\n",
      "    [  0.  56. 120. 136. 152.  80.   0.]\n",
      "    [  0.  72. 152. 168. 184.  96.   0.]\n",
      "    [  0.  52. 108. 116. 124.  64.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]]\n",
      "\n",
      "   [[  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   4.  12.  20.  28.  16.   0.]\n",
      "    [  0.  40.  88. 104. 120.  64.   0.]\n",
      "    [  0.  56. 120. 136. 152.  80.   0.]\n",
      "    [  0.  72. 152. 168. 184.  96.   0.]\n",
      "    [  0.  52. 108. 116. 124.  64.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]]\n",
      "\n",
      "   [[  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   4.  12.  20.  28.  16.   0.]\n",
      "    [  0.  40.  88. 104. 120.  64.   0.]\n",
      "    [  0.  56. 120. 136. 152.  80.   0.]\n",
      "    [  0.  72. 152. 168. 184.  96.   0.]\n",
      "    [  0.  52. 108. 116. 124.  64.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]]\n",
      "\n",
      "   [[  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   2.   6.  10.  14.   8.   0.]\n",
      "    [  0.  20.  44.  52.  60.  32.   0.]\n",
      "    [  0.  28.  60.  68.  76.  40.   0.]\n",
      "    [  0.  36.  76.  84.  92.  48.   0.]\n",
      "    [  0.  26.  54.  58.  62.  32.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]]\n",
      "\n",
      "   [[  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]\n",
      "    [  0.   0.   0.   0.   0.   0.   0.]]]]]\n",
      "Dilation test:\n",
      "Custom\n",
      "[[[[[ 64.  80.]\n",
      "    [192. 208.]]\n",
      "\n",
      "   [[ 64.  80.]\n",
      "    [192. 208.]]]\n",
      "\n",
      "\n",
      "  [[[ 64.  80.]\n",
      "    [192. 208.]]\n",
      "\n",
      "   [[ 64.  80.]\n",
      "    [192. 208.]]]]]\n",
      "Pytorch\n",
      "[[[[[ 64.  80.]\n",
      "    [192. 208.]]\n",
      "\n",
      "   [[ 64.  80.]\n",
      "    [192. 208.]]]\n",
      "\n",
      "\n",
      "  [[[ 64.  80.]\n",
      "    [192. 208.]]\n",
      "\n",
      "   [[ 64.  80.]\n",
      "    [192. 208.]]]]]\n",
      "Bias test:\n",
      "Custom\n",
      "[[[[[ 89. 105. 121.]\n",
      "    [121. 137. 153.]\n",
      "    [153. 169. 185.]]\n",
      "\n",
      "   [[ 89. 105. 121.]\n",
      "    [121. 137. 153.]\n",
      "    [153. 169. 185.]]\n",
      "\n",
      "   [[ 89. 105. 121.]\n",
      "    [121. 137. 153.]\n",
      "    [153. 169. 185.]]]\n",
      "\n",
      "\n",
      "  [[[ 89. 105. 121.]\n",
      "    [121. 137. 153.]\n",
      "    [153. 169. 185.]]\n",
      "\n",
      "   [[ 89. 105. 121.]\n",
      "    [121. 137. 153.]\n",
      "    [153. 169. 185.]]\n",
      "\n",
      "   [[ 89. 105. 121.]\n",
      "    [121. 137. 153.]\n",
      "    [153. 169. 185.]]]]]\n",
      "Pytorch\n",
      "[[[[[ 89. 105. 121.]\n",
      "    [121. 137. 153.]\n",
      "    [153. 169. 185.]]\n",
      "\n",
      "   [[ 89. 105. 121.]\n",
      "    [121. 137. 153.]\n",
      "    [153. 169. 185.]]\n",
      "\n",
      "   [[ 89. 105. 121.]\n",
      "    [121. 137. 153.]\n",
      "    [153. 169. 185.]]]\n",
      "\n",
      "\n",
      "  [[[ 89. 105. 121.]\n",
      "    [121. 137. 153.]\n",
      "    [153. 169. 185.]]\n",
      "\n",
      "   [[ 89. 105. 121.]\n",
      "    [121. 137. 153.]\n",
      "    [153. 169. 185.]]\n",
      "\n",
      "   [[ 89. 105. 121.]\n",
      "    [121. 137. 153.]\n",
      "    [153. 169. 185.]]]]]\n",
      "Padding mode test:\n",
      "Custom\n",
      "[[[[[  2.   6.  10.  14.   8.]\n",
      "    [ 20.  44.  52.  60.  32.]\n",
      "    [ 28.  60.  68.  76.  40.]\n",
      "    [ 36.  76.  84.  92.  48.]\n",
      "    [ 26.  54.  58.  62.  32.]]\n",
      "\n",
      "   [[  4.  12.  20.  28.  16.]\n",
      "    [ 40.  88. 104. 120.  64.]\n",
      "    [ 56. 120. 136. 152.  80.]\n",
      "    [ 72. 152. 168. 184.  96.]\n",
      "    [ 52. 108. 116. 124.  64.]]\n",
      "\n",
      "   [[  4.  12.  20.  28.  16.]\n",
      "    [ 40.  88. 104. 120.  64.]\n",
      "    [ 56. 120. 136. 152.  80.]\n",
      "    [ 72. 152. 168. 184.  96.]\n",
      "    [ 52. 108. 116. 124.  64.]]\n",
      "\n",
      "   [[  4.  12.  20.  28.  16.]\n",
      "    [ 40.  88. 104. 120.  64.]\n",
      "    [ 56. 120. 136. 152.  80.]\n",
      "    [ 72. 152. 168. 184.  96.]\n",
      "    [ 52. 108. 116. 124.  64.]]\n",
      "\n",
      "   [[  2.   6.  10.  14.   8.]\n",
      "    [ 20.  44.  52.  60.  32.]\n",
      "    [ 28.  60.  68.  76.  40.]\n",
      "    [ 36.  76.  84.  92.  48.]\n",
      "    [ 26.  54.  58.  62.  32.]]]\n",
      "\n",
      "\n",
      "  [[[  2.   6.  10.  14.   8.]\n",
      "    [ 20.  44.  52.  60.  32.]\n",
      "    [ 28.  60.  68.  76.  40.]\n",
      "    [ 36.  76.  84.  92.  48.]\n",
      "    [ 26.  54.  58.  62.  32.]]\n",
      "\n",
      "   [[  4.  12.  20.  28.  16.]\n",
      "    [ 40.  88. 104. 120.  64.]\n",
      "    [ 56. 120. 136. 152.  80.]\n",
      "    [ 72. 152. 168. 184.  96.]\n",
      "    [ 52. 108. 116. 124.  64.]]\n",
      "\n",
      "   [[  4.  12.  20.  28.  16.]\n",
      "    [ 40.  88. 104. 120.  64.]\n",
      "    [ 56. 120. 136. 152.  80.]\n",
      "    [ 72. 152. 168. 184.  96.]\n",
      "    [ 52. 108. 116. 124.  64.]]\n",
      "\n",
      "   [[  4.  12.  20.  28.  16.]\n",
      "    [ 40.  88. 104. 120.  64.]\n",
      "    [ 56. 120. 136. 152.  80.]\n",
      "    [ 72. 152. 168. 184.  96.]\n",
      "    [ 52. 108. 116. 124.  64.]]\n",
      "\n",
      "   [[  2.   6.  10.  14.   8.]\n",
      "    [ 20.  44.  52.  60.  32.]\n",
      "    [ 28.  60.  68.  76.  40.]\n",
      "    [ 36.  76.  84.  92.  48.]\n",
      "    [ 26.  54.  58.  62.  32.]]]]]\n",
      "Pytorch\n",
      "[[[[[  2.   6.  10.  14.   8.]\n",
      "    [ 20.  44.  52.  60.  32.]\n",
      "    [ 28.  60.  68.  76.  40.]\n",
      "    [ 36.  76.  84.  92.  48.]\n",
      "    [ 26.  54.  58.  62.  32.]]\n",
      "\n",
      "   [[  4.  12.  20.  28.  16.]\n",
      "    [ 40.  88. 104. 120.  64.]\n",
      "    [ 56. 120. 136. 152.  80.]\n",
      "    [ 72. 152. 168. 184.  96.]\n",
      "    [ 52. 108. 116. 124.  64.]]\n",
      "\n",
      "   [[  4.  12.  20.  28.  16.]\n",
      "    [ 40.  88. 104. 120.  64.]\n",
      "    [ 56. 120. 136. 152.  80.]\n",
      "    [ 72. 152. 168. 184.  96.]\n",
      "    [ 52. 108. 116. 124.  64.]]\n",
      "\n",
      "   [[  4.  12.  20.  28.  16.]\n",
      "    [ 40.  88. 104. 120.  64.]\n",
      "    [ 56. 120. 136. 152.  80.]\n",
      "    [ 72. 152. 168. 184.  96.]\n",
      "    [ 52. 108. 116. 124.  64.]]\n",
      "\n",
      "   [[  2.   6.  10.  14.   8.]\n",
      "    [ 20.  44.  52.  60.  32.]\n",
      "    [ 28.  60.  68.  76.  40.]\n",
      "    [ 36.  76.  84.  92.  48.]\n",
      "    [ 26.  54.  58.  62.  32.]]]\n",
      "\n",
      "\n",
      "  [[[  2.   6.  10.  14.   8.]\n",
      "    [ 20.  44.  52.  60.  32.]\n",
      "    [ 28.  60.  68.  76.  40.]\n",
      "    [ 36.  76.  84.  92.  48.]\n",
      "    [ 26.  54.  58.  62.  32.]]\n",
      "\n",
      "   [[  4.  12.  20.  28.  16.]\n",
      "    [ 40.  88. 104. 120.  64.]\n",
      "    [ 56. 120. 136. 152.  80.]\n",
      "    [ 72. 152. 168. 184.  96.]\n",
      "    [ 52. 108. 116. 124.  64.]]\n",
      "\n",
      "   [[  4.  12.  20.  28.  16.]\n",
      "    [ 40.  88. 104. 120.  64.]\n",
      "    [ 56. 120. 136. 152.  80.]\n",
      "    [ 72. 152. 168. 184.  96.]\n",
      "    [ 52. 108. 116. 124.  64.]]\n",
      "\n",
      "   [[  4.  12.  20.  28.  16.]\n",
      "    [ 40.  88. 104. 120.  64.]\n",
      "    [ 56. 120. 136. 152.  80.]\n",
      "    [ 72. 152. 168. 184.  96.]\n",
      "    [ 52. 108. 116. 124.  64.]]\n",
      "\n",
      "   [[  2.   6.  10.  14.   8.]\n",
      "    [ 20.  44.  52.  60.  32.]\n",
      "    [ 28.  60.  68.  76.  40.]\n",
      "    [ 36.  76.  84.  92.  48.]\n",
      "    [ 26.  54.  58.  62.  32.]]]]]\n"
     ]
    }
   ],
   "source": [
    "def test(own, torch, input, weight, bias):\n",
    "    torch.weight.data = from_numpy(weight).float()\n",
    "    torch.bias.data = from_numpy(bias).float()\n",
    "    own_result = np.floor(own)\n",
    "    torch_result = np.floor(torch(from_numpy(input).float()).detach().numpy())\n",
    "    print(\"Custom\")\n",
    "    print(own_result)\n",
    "    print(\"Pytorch\")\n",
    "    print(torch_result)\n",
    "    assert_array_equal(own_result, torch_result)\n",
    "    \n",
    "\n",
    "print(\"Zero inputs and weights\")\n",
    "inp, weight, bias = np.zeros((1, 1, 4, 4, 4)), np.zeros((2, 1, 2, 2, 2)), np.zeros((2,))\n",
    "own_result = conv3d(inp, weight, bias, in_channels=1, out_channels=2, kernel_size=2)\n",
    "torch_result = Conv3d(in_channels=1, out_channels=2, kernel_size=2)\n",
    "test(own_result, torch_result, inp, weight, bias)\n",
    "\n",
    "inp = np.array([[[[[1, 2, 3, 4], [9, 10, 11, 12], [5, 6, 7, 8],[13, 14, 15, 16]],\n",
    "                 [[1, 2, 3, 4], [9, 10, 11, 12], [5, 6, 7, 8],[13, 14, 15, 16]],\n",
    "                 [[1, 2, 3, 4], [9, 10, 11, 12], [5, 6, 7, 8],[13, 14, 15, 16]],\n",
    "                 [[1, 2, 3, 4], [9, 10, 11, 12], [5, 6, 7, 8],[13, 14, 15, 16]]], [[[1, 2, 3, 4], [9, 10, 11, 12], [5, 6, 7, 8],[13, 14, 15, 16]],\n",
    "                 [[1, 2, 3, 4], [9, 10, 11, 12], [5, 6, 7, 8],[13, 14, 15, 16]],\n",
    "                 [[1, 2, 3, 4], [9, 10, 11, 12], [5, 6, 7, 8],[13, 14, 15, 16]],\n",
    "                 [[1, 2, 3, 4], [9, 10, 11, 12], [5, 6, 7, 8],[13, 14, 15, 16]]]]])\n",
    "weight = np.array([[[[[1, 1],[1, 1]],[[1, 1],[1, 1]]], [[[1, 1],[1, 1]],[[1, 1],[1, 1]]]], [[[[1, 1],[1, 1]],[[1, 1],[1, 1]]], [[[1, 1],[1, 1]],[[1, 1],[1, 1]]]]])\n",
    "bias = np.array([0, 0])\n",
    "\n",
    "print(\"Default data\")\n",
    "own_result = conv3d(inp, weight, bias, in_channels=2, out_channels=2, kernel_size=2)\n",
    "torch_result = Conv3d(in_channels=2, out_channels=2, kernel_size=2)\n",
    "test(own_result, torch_result, inp, weight, bias)\n",
    "\n",
    "print(\"Random data\")\n",
    "np.random.seed(42)\n",
    "inp = np.random.rand(1, 2, 5, 5, 5)\n",
    "weight = np.random.rand(2, 2, 2, 2, 2)\n",
    "bias = np.random.rand(2)\n",
    "own_result = conv3d(inp, weight, bias, in_channels=2, out_channels=2, kernel_size=2)\n",
    "torch_result = Conv3d(in_channels=2, out_channels=2, kernel_size=2)\n",
    "test(own_result, torch_result, inp, weight, bias)\n",
    "\n",
    "inp = np.array([[[[[1, 2, 3, 4], [9, 10, 11, 12], [5, 6, 7, 8],[13, 14, 15, 16]],\n",
    "                 [[1, 2, 3, 4], [9, 10, 11, 12], [5, 6, 7, 8],[13, 14, 15, 16]],\n",
    "                 [[1, 2, 3, 4], [9, 10, 11, 12], [5, 6, 7, 8],[13, 14, 15, 16]],\n",
    "                 [[1, 2, 3, 4], [9, 10, 11, 12], [5, 6, 7, 8],[13, 14, 15, 16]]], [[[1, 2, 3, 4], [9, 10, 11, 12], [5, 6, 7, 8],[13, 14, 15, 16]],\n",
    "                 [[1, 2, 3, 4], [9, 10, 11, 12], [5, 6, 7, 8],[13, 14, 15, 16]],\n",
    "                 [[1, 2, 3, 4], [9, 10, 11, 12], [5, 6, 7, 8],[13, 14, 15, 16]],\n",
    "                 [[1, 2, 3, 4], [9, 10, 11, 12], [5, 6, 7, 8],[13, 14, 15, 16]]]]])\n",
    "weight = np.array([[[[[1, 1],[1, 1]],[[1, 1],[1, 1]]], [[[1, 1],[1, 1]],[[1, 1],[1, 1]]]], [[[[1, 1],[1, 1]],[[1, 1],[1, 1]]], [[[1, 1],[1, 1]],[[1, 1],[1, 1]]]]])\n",
    "bias = np.array([0, 0])\n",
    "    \n",
    "print(\"Stride test:\")\n",
    "own_result = conv3d(inp, weight, bias, in_channels=2, out_channels=2, kernel_size=2, stride=2)\n",
    "torch_result = Conv3d(in_channels=2, out_channels=2, kernel_size=2, stride=2)\n",
    "test(own_result, torch_result, inp, weight, bias)\n",
    "\n",
    "print(\"Kernel size test:\")\n",
    "own_result = conv3d(inp, weight, bias, in_channels=2, out_channels=2, kernel_size=2)\n",
    "torch_result = Conv3d(in_channels=2, out_channels=2, kernel_size=2)\n",
    "test(own_result, torch_result, inp, weight, bias)\n",
    "\n",
    "print(\"Padding test:\")\n",
    "own_result = conv3d(inp, weight, bias, in_channels=2, out_channels=2, kernel_size=2, padding=1)\n",
    "torch_result = Conv3d(in_channels=2, out_channels=2, kernel_size=2, padding=1)\n",
    "test(own_result, torch_result, inp, weight, bias)\n",
    "\n",
    "print(\"Padding test:\")\n",
    "own_result = conv3d(inp, weight, bias, in_channels=2, out_channels=2, kernel_size=2, padding=2)\n",
    "torch_result = Conv3d(in_channels=2, out_channels=2, kernel_size=2, padding=2)\n",
    "test(own_result, torch_result, inp, weight, bias)\n",
    "\n",
    "print(\"Dilation test:\")\n",
    "own_result = conv3d(inp, weight, bias, in_channels=2, out_channels=2, kernel_size=2, dilation=2)\n",
    "torch_result = Conv3d(in_channels=2, out_channels=2, kernel_size=2, dilation=2)\n",
    "test(own_result, torch_result, inp, weight, bias)\n",
    "\n",
    "print(\"Bias test:\")\n",
    "own_result = conv3d(inp, weight, bias=[1, 1], in_channels=2, out_channels=2, kernel_size=2)\n",
    "torch_result = Conv3d(in_channels=2, out_channels=2, kernel_size=2, bias=True)\n",
    "test(own_result, torch_result, inp, weight, bias=np.array([1,1]))\n",
    "\n",
    "print(\"Padding mode test:\")\n",
    "own_result = conv3d(inp, weight, bias, in_channels=2, out_channels=2, kernel_size=2, padding=1, padding_mode='zeros')\n",
    "torch_result = Conv3d(in_channels=2, out_channels=2, kernel_size=2, padding=1, padding_mode='zeros')\n",
    "test(own_result, torch_result, inp, weight, bias)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996188a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
